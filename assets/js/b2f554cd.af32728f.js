"use strict";(self.webpackChunkmi_x_lab=self.webpackChunkmi_x_lab||[]).push([[894],{6042:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/RouteNet-Fermi/RouteNet-Fermi","metadata":{"permalink":"/blog/RouteNet-Fermi/RouteNet-Fermi","source":"@site/blog/RouteNet-Fermi/RouteNet-Fermi.md","title":"RouteNet-Fermi, Network Modeling with Graph Neural Networks","description":"Authors","date":"2024-03-14T14:03:52.000Z","tags":[{"label":"GNN","permalink":"/blog/tags/gnn"},{"label":"Network-Modeling","permalink":"/blog/tags/network-modeling"}],"readingTime":45.725,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"RouteNet-Fermi, Network Modeling with Graph Neural Networks","date":"2024-03-14T14:03:52.000Z","tags":["GNN","Network-Modeling"],"categories":"\u8bba\u6587"},"unlisted":false,"nextItem":{"title":"Awesome things to do","permalink":"/blog/welcome"}},"content":"**Authors**\\n\\nMiquel Ferriol-Galm\xe9s, Jordi Paillisse, Jos\xe9 Su\xe1rez-Varela, Krzysztof Rusek, Shihan Xiao, Xiang Shi, Xiangle Cheng, Pere Barlet-Ros, Albert Cabellos-Aparicio\\n\\n---\\n\\n## Abstract\\n\\nNetwork models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models presents limitations, such as the assumption of Markovian traffic in queueing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks(GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviours. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goal as queuing theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles - e.g., with complex non-Markovian models - and arbitrary routing and queue scheduling configurations. Our experimental results show that RouteNet-Fermi achieves similar accuracy as computationally-expensive packet-level simulators and scales accurately to larger networks. Our model produces delay estimates with a mean relative error of 6.24% when applied to a test dataset of 1,000 examples, including network topologies one order of magnitude larger than those seen during training. Finally, we have also evaluated RouteNet-Fermi with measurements from a physical testbed and packet traces from a real-life network.\\n\\n\x3c!-- truncate --\x3e\\n\\n**Index terms:**\xa0Network Modeling, Graph Neural Networks, Queuing Theory\\n\\n## Introduction\\n\\nNetwork modeling is arguably one of the key tools when designing, building, and evaluating computer networks, even since the early days of networking. Network models are used in protocol design, performance evaluation, or network planning, just to cite a few examples. The two most widespread network modeling techniques are analytical models based on Queuing Theory (QT), and packet-level simulators.\\n\\nHowever, the evolution of computer networks especially concerning complexity traffic characteristics, highlights some of the limitations of classical modeling techniques. Despite the tremendous success and widespread usage, some scenarios require more advanced techniques capable of accurately modeling complex traffic characteristics, while scaling to large real-world networks.\\n\\nEspecially, two relevant applications can benefit from advanced network modeling techniques: Network Digital Twins (NDT), and the network optimization tools. Commonly, an NDT is referred to as a virtual replica of a physical network that can accurately mimic its behavior and can make performance predictions for any given input condition (e.g., traffic, topology change, or new routing configuration). In other words, an NDT is an accurate network model that can support a wide range of network configurations and that can accurately model the complex non-linear behaviors behind real-world networks. As a result, NDTs can be used to produce accurate performance predictions, carry out what-if analysis, or perform network optimization by pairing it with an optimization algorithm.\\n\\nIn the context of network optimization, we can only optimize what we can model.\xa0Optimization algorithm operate by searching the network configuration space\xa0(e.g., to find an alternative routing scheme). For each configuration, a network model is used to estimate the resulting performance to see if it fulfills the optimization goal (e.g., minimize delay). To achieve efficient online optimization, it is essential to have an accurate and fast network model.\\n\\nState-of-the-art modeling techniques have important limitations in effectively supporting the stringent requirements of current packet-switched networks. Queuing Theory imposes strong assumptions on the packet arrival process (Poisson traffic generation), which often is not sufficient to model real-world networks. Internet traffic has been extensively analyzed in the past two decades, and despite the community has not agreed on universal model, there is consensus that in general aggregated traffic shows strong auto-correlation and a heavy-tail.\\n\\nAlternatively, packet-level simulators can accurately model networks. However, this comes at a high computational cost. The cost of a simulator depends linearly on the number of packets forwarded, which can be in the range of millions per second on a single 1Gbps link. In consequence, they are slow and impractical when considering large networks with realistic traffic volumes. This also severely limits its applicability to online network optimization, given the hard time constraints of such type of applications.\\n\\nIn this context, Deep Learning (DL) offers an extraordinary set of techniques to build accurate data-driven network models. DL models cam be trained with real data, without making any assumptions about physical networks. This enables building models with unprecedented accuracy by modeling the entire range of non-linear and multidimensional characteristics.\\n\\nIn this paper, we first make a systematic analysis of the performance of DL techniques for network modeling, using classical discrete-event network simulators as baseline. Specifically, we analyse the performance of Multilayer Perceptron-based (MLP), Recurrent Neural Network-based (RNN), and Graph Neural Network-based (GNN) models. We find that classical DL techniques, such as MLPs and RNNs are not practical enough for network modeling as they fail to provide accurate estimates when the network scenario differs from the examples seen during training (e.g., link failure). More recently, GNNs have been proposed as a novel neural network architecture specifically designed to learn over graph-structured data. They have been successfully used in other domains, such as quantum chemistry or logistics. However, in our analysis, we find that standard GNNs do not work well for network modeling and that we need a custom GNN architecture to model computer networks.\\n\\nAs a result, we propose RouteNet-Fermi (RouteNet-F), a GNN architecture for network modeling. RouteNet-F shares the same goals as Queuing Theory. It provides performance estimates (delay, jitter, and packet-loss) on given network scenarios (**Figure 1**) with remarkable accuracy and speed. The proposed model is not limited to Markovian traffic as Queuing Theory; it supports arbitrary models (including auto-correlated processes) which better represent the properties of real-world traffic. Interestingly, it also overcomes one of the main limitations of DL-based models: RouteNet-F generalises and provides accurate estimates in network scenarios not seen in training (e.g., different topologies, traffic metrics, routing configurations). We benchmark RouteNet-F against a state-of-the-art DL-based model (MimicNet) and with a state-of-the-art queuing theory model. We show that our model outperforms both baselines in all scenarios, achieving a 5.64% error when tested in a dataset with packet traces coming from a real-world network, an 11% error when evaluated in a physical testbed, and a 6.24% error when estimating the delay on networks over a large dataset with 1,000 network examples, with topologies ranging from 50 to 300 nodes.\\n\\n\\n![Black-box representation of RouteNet-F](img/Fig_1.png)\\n*Fig. 1 Black-box representation of RouteNet-F*\\n\\nAs any Deep Learning model, RouteNet-Fermi does not provide strong mathematical performance guarantees. However, the error of the estimates produced by the model is strongly bounded. The minimum estimated delay assumes no queuing across the path while the maximum assumes that all the queues are full. RouteNet-Fermi will not produce delay estimates outside these bounds.\\n\\nThe implementation of the model used in the evaluation in this paper is publicly available at\xa0[RouteNet-Fermi](https://github.com/BNN-UPC/Papers/wiki/RouteNet_Fermi).\\n\\n## Challenges of Data-Driven Network Modeling\\n\\nThis section describes the main challenges that data-driven solutions need to address for network modeling. These challenges drove the core design of RouteNet-Fermi.\\n\\n**Traffic Models:** Networks carry different types of traffic, so, supporting arbitrary stochastic traffic model is crucial. Experimental observations show that traffic on the Internet has strong auto-correlation and heavy-tails. In this context, it is well-known that main limitation of Queuing Theory is that it fails to provide accurate estimates on realistic Markovian models with continuous state space, or non-Markovian traffic models. The challenge for DL-based modeling is: How can we design a neural network architecture that can accurately model realistic traffic models.\\n\\n**Training and Generalisation:** One of the main differences between analytical modeling (e.g., QT) and data-driven modeling is that the latter requires training. In DL, training involves obtaining a representative dataset of network measurements. The dataset needs to include a broad spectrum of network operational regimes, ranging from different congestion levels to various routing configurations, among others. In other words, the DL model cam predict only scenarios it has previously seen. Note that this is a common property of all neural network architectures.\\n\\nIdeally, we would obtain this training dataset from a production network, since they commonly have systems in place to measure performance. However, it would be difficult to obtain a representative dataset. As we mentioned previously, we would need to measure the production network when it is experiencing extreme performance degradation as the result of link failures, incorrect configurations, severe congestion, etc. However, these situations are not common in production networks, which limits the ability to generate the training dataset. A reasonable alternative is creating these datasets in controlled testbeds, where it is possible to use different traffic models, implement a broad set of configurations, and replicate a wide range of network failures. Thus, the DL model can be trained on samples from testbed and then, applied to production networks. Hence, the research challenge is: how to design a DL model that can provide accurate estimates in network not seen during training? This incudes topologies, traffic, and configurations (e.g., queuing scheduling, routing) different from those seen in the training network testbed.\\n\\nLeveraging a testbed that is smaller than a production network creates another challenge: the generalisation to larger networks. Real-world networks include hundreds or thousands of nodes, and building a network testbed at this scale is typically unfeasible. As a result, the Dl model should be able to learn from datasets with samples of small network testbeds and predict metrics for considerably larger networks, e.g., by a factor of 10-100x. Generalising to larger networks, or graphs in general, is currently an open research challenge in the field of GNNs.\\n\\n**Quality of Service and Scheduling Policies:** A key requirement of modern networks is supporting Quality of Service (QoS), usually implemented via scheduling policies and mapping of traffic flows to QoS classes. Hence, a DL model should be able to predict the performance of the input traffic flows with their associated QoS class, similarly to how QT models support a wide range of scheduling policies.\\n\\n## Limitations of Current Network Modeling Techniques\\n\\nThis section explores the performance of different DL models with respect to an accurate packet-level simulator and discusses the main limitations of existing network modeling techniques.\\n\\n### Simulations as Network Modeling Technique\\n\\nNetwork simulators reproduce the network behaviour at the granularity of packet events. This way, they can offer excellent accuracy and can be easily extended to include virtually and feature, such as packet scheduling, wireless networks, etc. Some simulators, such as Omnet++ or ns-3, are widely used and maintained.\\n\\nHowever, their main limitation is the simulation time, especially for networks with high-speed links (10Gbps and above). Hence, depending on the amount of the traffic found in the target network, it may become unfeasible to simulate the network.\\n\\nTo illustrate this limitation, we simulate different topologies using the Omnet++ simulator to calculate the delay of a set of source-destination flows (CPU Intel Xeon Silver 4210R @2.40GHz). Network topologies are artificially generated using the Power-Law Out-Degree Algorithm from [this paper](https://ieeexplore.ieee.org/abstract/document/892042) and a traffic distribution that follows a Poisson process.\\n\\n**Figure 2** shows the simulation time of such networks depending on the number of events. Here, an event refers to a transition in status of the network (e.g., adding a new packet to queue). We can see that the simulation time increases linearly and that simulating 4 billion events may appear a larger figure, consider that a 10Gbps link transmitting regular Ethernet frames translates to $\\\\approx 820k$ events per seconds or 247 million events in 5 minutes of network activity for a single link. For example, in our experiments, the simulator takes around 8h to compute the performance metrics of a 300-node network.\\n\\n![Simulation time depending on the number of processed events.](img/Fig_2.png)\\n*Fig. 2 Simulation time depending on the number of processed events.*\\n\\nSo, the main limitation of packet-level simulators is the simulation time. On the contrary, packet simulators offer unrivalled accuracy and can simulate virtually any scenario, from different routing configurations to replaying packet traces to simulate unknown traffic models. Because of this, hereafter, we consider the results from the simulator as the ground truth for the evaluations in this paper.\\n\\n### Neural Networks as Network Modeling Techniques\\n\\nThe following sections review the performance of three common Neural Network (NN) architectures in the order of increasing complexity. First, we evaluate the Multilayer Perceptron, one of the simplest NNs. Next, Recurrent Neural Networks which are designed to work with sequences. Finally, we directly input the network into a Graph Neural Network specifically designed to work with graphs. The objective is to create a network model with the NN that can predict performance parameters for input networks with a wide range of characteristics. We are especially interested in the following parameters:\\n\\n- **Accuracy:** How close is the prediction to simulation values?\\n- **Different Routing:** Does the accuracy degrade if we change the routing configuration?\\n- **Link Failures:** Quantify if link failures affect the quality of predictions?\\n\\nWe trained and test the three neural networks with the same dataset, obtained from simulations with Omnet++. The input values are the network characteristics (topology, routing configuration, traffic model and intensity, etc), and the output values are the delay for each path. Hence, all the errors are computed with respect to values of the simulator. We use four different datasets:\\n\\n- **Traffic Models:** In it, we consider traffic models that are non-Poisson, auto-correlated, and with heavy tails. Table IV details the different traffic models.\\n- **Same Routing:** Where the testing and training datasets contain networks with the same routing configurations.\\n- **Different Routing:** Where the training and testing datasets contain networks with different routing configurations.\\n- **Link failures:** Here, we iteratively remove one link of the topology to replicate a link failure, until we transform the network graph into a connected acyclic graph. This scenario is the most complex since a link failure triggers a change both in the routing and the topology.\\n\\nTo compare the different techniques, we compute the prediction error with respect to the accurate performance values produced by the simulator. Particularly we use the following error metrics:\\n\\n- Mean Absolute Percentage Error (MAPE),\\n- Mean Squared Error (MSE),\\n- Mean Absolute Error (MAE), and\\n- Coefficient of Determination ($R^2$).\\n\\n### Multilayer Perceptron\\n\\nA Multilayer Perceptron (MLP) is a basic kind of NN from the family of feedforward NNs. In short, input data is propagated unidirectionally from the input neuron layer to the output layer. There may be an arbitrary number of hidden layers between these two layers, and this determines how deep is the NN.\\n\\n- **Design:** Several works have leveraged an MLP to predict network performance metrics. Based on this work, we have built an MLP to predict the mean delay for each source-destination pair of nodes of a given network. The MLP has 8,280 inputs and two hidden layers with 4096 neurons and uses Rectified Linear Units (ReLU) as activation functions.\\n- **Evaluation:** **Table I** presents the error when predicting the network delay with respect to the results produced by the network simulator, including several traffic models. We can see that the MLP offers good accuracy for Poisson traffic, but the error increases significantly for the rest of the traffic models showing a MAPE between 23% and 84%.\\n\\n![](img/Table_I.png)\\n*Table. I Delay prediction using an MLP and an RNN for different traffic models. The error is computed w.r.t simulation results.*\\n\\n![Delay prediction using an MLP and an RNN for the same and different routing configurations w.r.t those seen during training, and considering various link failures. The error is relative to simulation results.](img/Table_II.png)\\n*Table. II Delay prediction using an MLP and an RNN for the same and different routing configurations w.r.t those seen during training, and considering various link failures. The error is relative to simulation results.*\\n![[Fig_3.png]]\\nLikewise, **Table II** shows the error of predicting the delay for the datasets with the same/different routing and link failures. We can see that the MLP cannot offer an accurate estimate when predicting the delay of a previously unseen routing configuration (1150% of error). This is due to the internal architecture of the MLP. During training, the MLP performs overfitting, meaning that the model only learns about the initial network topology used for training and not for any others. When we input a new topology, it does not have sufficient information to make an accurate prediction.\\n\\n### Recurrent Neural Networks\\n\\nRecurrent Neural Networks (RNN) are a more advanced type of NN. They have shown excellent performance when processing sequential data. This is mainly because they connect some layers to the previous ones, which gives them the ability to keep the state along sequences.  \\n\\n![Sample Topology with 5 nodes, three links, and two flows.](img/Fig_3.png)\\n*Fig. 3 Sample Topology with 5 nodes, three links, and two flows.*\\n\\n![Recurrent Neural Network model for the Sample Topologys](img/Fig_4.png)\\n*Fig. 4 Recurrent Neural Network model for the Sample Topology*\\n\\n- **Design:** Several works propose RNNs as a way to predict network performance. In this experiment, we build a sequential model with an RNN (**Figure 4**). Particularly, we choose a Gated Recurrent Unit (GRU). We initialise the state of each path with the sequence of nodes in the path and the features of the traffic model (e.g., packets, bandwidth, $\\\\lambda$, $\\\\epsilon$, $\\\\alpha$, or on-off time), and we update the state of each link across the path. As an example, Figure 4 shows the structure of an RNN to model the sample network from **Figure 3**. We can see that the path of $flow_1$ is composed of $L_1$ and $L_3$. Once the path state has been computed, an MLP with 2 hidden layers computes the final output.\\n- **Evaluation:** We train the RNN with the same datasets as the previous subsection. Although the RNN supports better different traffic models than the MLP (**Table I**), it still struggles to produce accurate predictions when there are routing or topology changes (**Table II**), especially for different routing configurations (30% error), or when removing links (63%).\\n\\nThe reason behind the lack of capability of RNNs to understand routing changes and link failures is due to its internal architecture. RNNs can accommodate different end-to-end paths in the network (i.e., series of routers and links), thereby, making it easier to perform predictions for paths never seen in the training phase. However, this structure cannot store and update the status of individual links in the topology due to the inter-dependency between links and traffic flows (i.e., routing). In other words, if the status of a link changes, it affects several flows, and vice-versa. This generates circular dependencies that RNNs are not able to model (see more details in **Sec. IV-C**).\\n\\n### Graph Neural Networks\\n\\nNetworks are fundamentally represented as graphs, where networking devices are the graph nodes and the links connecting devices are the graph edges. This interconnection translates to the fact that the **different elements in the network are dependent on each other**. Since most standard DL models (e.g., MLP, RNN) assume independent flow-level data points. This renders them inaccurate for our use case. Hence, a model that is capable of processing directly the network graph is arguably more desirable for network modeling, because **it will be able, not only to obtain information from the individual nodes and edges but also from the underlying data structure** (i.e., the relationships between the different elements).\\n\\n![](img/Table_III.png)\\nGNNs are a type of neural network designed to work with graph-structured data. They have two key characteristics that make them a good candidate for a network model. First, GNNs have the ability to store node-level hidden states and update them in each iteration. Second, they process the input data directly as a graph, both during training and inference. This means that the internal structure of the neural network depends on the input graph. Hence, they can dynamically adapt to the underlying dependencies between the different elements of a network. The latter is of special importance, since changes in the network graph (e.g., routing modifications, link failures) are the main limitation of other DL-based models, such as MLPs and RNNs, as we have seen in the previous subsections.\\n\\nIn this section, we build a standard GNN model to predict the mean per-flow delay in networks.\\n\\n**Design:** We implement a Message-Passing Neural Network (MPNN), a powerful state-of-the-art GNN architecture that can efficiently capture dependencies between the elements of input graphs. We define a graph $G$ described as a set of vertices (or nodes) $E$ and a set of edges (or links) $V$. Each node has a set of features $x_v$, and edges also have some features $e_{vw}$. The execution of an MPNN can be divided into three phases, an initialisation phase, a message-passing phase, and a readout phase. The first one defines a hidden state ($h_v^0$) using the node features ($x_v$). The second one is an iterative process that runs for $T$ time steps and that is defined by two functions: the message function $M_t$ and the update function $U_t$. During this phase, node hidden states ($h_v^t$), represented as fixed-size vectors, attempt to encode some meaningful information, and are iteratively updated by exchanging messages $m_v^{t+1}$ with their neighbours:\\n\\n$$\\nm_v^{t+1} = \\\\sum_{w \\\\in N(v)}{M_t(h_v^t,h_w^t,e_{vw})}\\\\quad(1)\\n$$\\n$$\\nh_v^{t+1} = U_t(h_v^t,m_v^{t+1})\\\\quad(2)\\n$$\\n\\nwhere $N(v)$ represents the neighbours of $v$ in graph $G$. Finally, the readout phase computes the output vector using a readout function $R$ that takes as input the final hidden states $h_v^T$:  \\n\\n$$\\n\\\\hat{y} = R(\\\\{h_v^T | v \\\\in G\\\\})\\\\quad(3)\\n$$\\n\\nIn our case, the input of the MPNN is the network topology graph. It performs $T$$=$$4$ message-passing iterations and the hidden state dimension is 32. The readout function is implemented with a two-layer fully connected NN with ReLUs as activation functions for the hidden layers and a linear activation for the output layer.\\n\\n![Delay prediction using an MPNN for the same and different routing configurations w.r.t those seen during training, and considering various link failures. The error is relative to simulation results.](img/Table_III.png)\\n*Table. III Delay prediction using an MPNN for the same and different routing configurations w.r.t those seen during training, and considering various link failures. The error is relative to simulation results.*\\n\\n**Evaluation:** We evaluate the accuracy of the MPNN model when predicting the mean flow-level delay, as in previous subsections. Table III presents the delay prediction errors in the same scenarios of the previous experiments: routing configurations, both seen and not seen during training, and link failures. Unfortunately, the results are similar to those of the RNN: the routing configurations from the training dataset are easy to predict, with an error as low as 3%, while new routing configurations and link failures increase the error significantly (respectively, 50% and 125% of MAPE), thus showing even larger errors than the RNN model.  \\n\\nThe main reason behind the poor accuracy of the MPNN model is that the architecture of this model is directly built based on the network topology without taking into account the paths traversed by different traffic flows (i.e., the routing configuration), which is a fundamental property to understand inter-dependencies between flows and links. More specifically, when we test the model with the same routing configuration, it learns the relationships between flows and links. However, when we change this configuration, those previously-learned relationships are no longer valid, and the model is not able to capture the relationships between elements in the new scenario.\\n\\nThis intuition is better understood with an extreme packet loss example. Let\u2019s suppose we have the sample network in Figure 3. The first flow ($Flow_1$) is transmitting at a rate of 2Gbps, and the second one ($Flow_2$) is transmitting at a rate of 0.5Gbps. As we can see, $L_1$ has a maximum capacity of 0.5Gbps. Because of this, $Flow_1$ will experience a large packet loss, which causes the traffic of $Flow_1$ at $L_3$ to be at most 0.5Gbps. Hence, instead of having 2Gbps+0.5Gbps of aggregated traffic at $L_3$, it will only have 1Gbps. Now, $Flow_2$, which initially could have experienced a lot of network congestion when going through the 2Gbps link will experience none as the state of the link changed.\\n\\nKnowing this, we can see how there is a circular dependency between the flows and links found in the network. At the same time, the state of a flow depends on the state of the links they traverse, and the state of the links depends on the state of the flows passing through them.\\n\\nIf we apply a standard GNN over this example, the state of each flow is not updated at each hop. Therefore, the GNN does not have a structure that represents how the delay depends on both the links (topology) and the flows that go through each specific router.\\n\\nHence, we conclude that feeding the MPNN directly with the network topology graph is not sufficient to accurately perform network modeling. However, more complex and customised GNN-based architectures can still be powerful for modeling the inter-dependencies between the different network elements and generalising over new network scenarios by exploiting the underlying graph structure.\\n\\n## RouteNet-Fermi\\n\\nThis section describes the internal GNN architecture of RouteNet-Fermi (hereafter referred to as RouteNet-F). This GNN-based model implements a custom three-stage message-passing algorithm that represents key elements for network modeling (e.g., topology, queues, traffic flows). RouteNet-F supports a wide variety of features present in real-world networks, such as multi-queue QoS scheduling policies or complex traffic models.\\n\\n**Figure 1** shows a black-box representation of RouteNet-F. The input of this model is a network sample, defined by: a network topology, a routing scheme (flow level), a queuing configuration (interface level), and a set of traffic flows characterised by some parameters. As output, the model produces estimates of relevant performance metrics at a flow-level granularity (e.g., delay, jitter, packet loss). **Figure 5** shows the end-to-end workflow to train, validate, and use RouteNet-Fermi.\\n\\n![End-to-end workflow of RouteNet-F. 1) Collection of small-scale observations coming from a controlled environment, 2) Model training, 3) Model testing with various configurations (e.g., routing, scheduling) never seen during the training phase, 4) hyper-parameter tunning to balance highly accurate predictions with performance, 5) Simulation of large-scale production networks. One of the main advantages of RouteNet-F is that usually time-consuming steps like 1), 2), 3), and 4) are all done at small scales and, therefore, are fast as well.](img/Fig_5.png)\\n*Fig. 5 End-to-end workflow of RouteNet-F. 1) Collection of small-scale observations coming from a controlled environment, 2) Model training, 3) Model testing with various configurations (e.g., routing, scheduling) never seen during the training phase, 4) hyper-parameter tunning to balance highly accurate predictions with performance, 5) Simulation of large-scale production networks. One of the main advantages of RouteNet-F is that usually time-consuming steps like 1), 2), 3), and 4) are all done at small scales and, therefore, are fast as well.*\\n\\n### Model Description\\n\\nRouteNet-F is based on two main design principles: $(i)$ finding a good representation of the network components supported by the model (e.g., traffic models, routing, queue scheduling), and $(ii)$ exploit scale-independent features of networks to accurately scale to larger networks unseen during training. These two aspects are further discussed in the next two subsections.\\n\\n### Representing network components and their relationships\\n\\nFirst, let us define a network as a set of source-destination flows $\\\\mathcal{F}=\\\\{f_i:i\\\\in(1,...,n_f)\\\\}$, a set of queues on $\\\\mathcal{Q}=\\\\{q_j: j \\\\in (1,...,n_q)\\\\}$, and a set of links $\\\\mathcal{L} = \\\\{l_k: k\\\\in(1,...,n_l)\\\\}$. According to the routing configuration, flows follow a source-destination path. Hence, we define flows as sequences of tuples with the queues and links they traverse $f_i=\\\\{(q^{i,1},l^{i,1}),...,(q^{i,M},l^{i,M})\\\\}$, where $M$ is the path length of the flow (number of links). Let us also define $Q_f(q_j)$ and $L_f(l_k)$ as functions that respectively return all the flows passing through a queue $q_j$ or a link $l_k$. Also, $L_q(l_k)$ is defined as a function that returns the queues $q_{l_k}\\\\in \\\\mathcal{Q}$ injecting traffic into link $l_k$ (i.e., the queues at the output port to which the link is connected).\\n\\nFollowing the previous notation, RouteNet-F considers an input graph with three main components: $(i)$ the physical links $\\\\mathcal{L}$ that shape the network topology, $(ii)$ the queues $\\\\mathcal{Q}$ at each output port of network devices, and $(iii)$ the active flows $\\\\mathcal{F}$ in the network, which follow some specific src-dst path (i.e., sequences of queues and links). Traffic in flows is generated from a given traffic model. From this, we can extract three basic principles:\\n\\n- The state of flows (e.g., delay, throughput, packet loss) is affected by the state of the queues and links they traverse (e.g., queue/link utilization).\\n- The state of queues (e.g., occupation) depends on the state of the flows passing through them (e.g., traffic volume, burstiness).\\n- The state of links (e.g., utilization) depends on the states of the queues that can potentially inject traffic into the link, and the queue scheduling policy applied over these queues (e.g., Strict Priority, Weighted Fair Queuing).\\n\\nFormally, these principles can be formulated as follows:\\n\\n$$h_{f_i}=G_f(h_{q^{i,1}},h_{l^{i,1}},...,h_{q^{i,M}},h_{l^{i,M}})\\\\quad(4)$$\\n\\n$$h_{q_j} = G_q(h_{f_1},...,h_{f_I}), \\\\quad f_i \\\\in Q_f(q_j)\\\\quad(5)$$\\n\\n$$h_{l_k} = G_l(h_{q_1},...,h_{q_J}), \\\\quad q_j \\\\in L_q(l_j)\\\\quad(6)$$\\n\\nWhere $G_f$, $G_q$, and $G_l$ are some unknown functions, and $h_f$, $h_q$ and $h_l$ are latent variables that encode information about the state of flows $\\\\mathcal{F}$, queues $\\\\mathcal{Q}$, and links $\\\\mathcal{L}$ respectively. Note that these principles define a circular dependency between the three network components ($\\\\mathcal{F}$, $\\\\mathcal{Q}$, and $\\\\mathcal{L}$) that must be solved to find latent representations satisfying the equations above.\\n\\nTo solve the circular dependencies defined in Equations $(4)-(6)$, RouteNet-F implements a three-stage message passing algorithm that combines the states of flows $\\\\mathcal{F}$, queues $\\\\mathcal{Q}$, and links $\\\\mathcal{L}$, and updates them iteratively. Finally, it combines these states to estimate flow-level delays, jitters, and packet loss. **Figure 6** shows a schematic representation of the internal three-stage message-passing architecture of this model.\\n\\n![Schematic representation of RouteNet-F.](img/Fig_6.png)\\n*Fig. 6 Schematic representation of RouteNet-F.*\\n\\n**Algorithm 1** describes the architecture of RouteNet-F. First, hidden states $h_f$, $h_q$, and $h_l$ are initialised using the functions $HS_f$, $HS_q$, and $HS_l$ respectively (lines 1-3). These functions encode the initial features $x_{f}$, $x_{q}$, and $x_{l}$ into fixed-size vectors that represent feature embeddings. The initial features of flows $x_f$ are defined as an n-element vector that characterises the flow\'s traffic. For example, in our case, this vector includes the average traffic volume transmitted in the flow $\\\\lambda$, and some specific parameters of the traffic model, such as $t_{on}$ and $t_{off}$ for On-Off traffic distributions or $\\\\alpha$ and $\\\\beta$ for exponential models. We set the initial features of links $x_l$ as $(i)$ the link load $x_{l_{load}}$, and $(ii)$ the scheduling policy at the output port of the link (FIFO, Strict Priority, Weighted Fair Queuing, or Deficit Round Robin). For the scheduling policy, we use a one-hot encoding. The calculation of the link load $x_{l_{load}}$ is defined in more detail later (**Sec. IV-C**). Lastly, the initial features of queues $x_q$ include: $(i)$ the buffer size, $(ii)$ the queue order/priority level (one-hot encoding), and $(iii)$ the weight (only for Weighted Fair Queuing or Deficit Round Robin configurations).\\n\\nOnce all the hidden states are initialised, the message-passing phase starts. This phase is executed for $T$ iterations (loop from line 4), where $T$ is a configurable parameter of the model. Each message passing iteration is divided into three stages, which represent respectively the message exchanges and updates of the hidden states of flows $h_f$ (lines 5-10), queues $h_q$ (lines 11-14), and links $h_l$ (lines 15-19).\\n\\nFinally, the loop from line 20 computes the different flow-level performance metrics. Here, function $R_{f_d}$ (line 24) and $R_{f_j}$ (line 28) represent a readout function that is individually applied to the hidden states of flows as they pass through a specific link ($h_{f,l}$). The output of these functions is the average queue occupancy and the delay variation (i.e., jitter) seen by the flow at that link. Note that different flows may experience different queue occupancies and jitter depending on their traffic properties (e.g., traffic volume, burstiness). Lastly, these link-level delay and jitter estimates are combined to compute the final flow-level delay $\\\\hat{y_{f_d}}$ and jitter $\\\\hat{y_{f_j}}$. This calculation is further described in **Section IV**. Similarly, $R_{f_l}$ (line 29) is applied to the hidden states of flows $h_f$ to compute the per-flow packet loss rate.\\n\\n![](img/Algorithm_1.png)\\n\\nData-driven models typically need to see edge cases that are uncommon in real-world production networks (e.g., link failures). This means that collecting data directly from production networks requires testing configurations that might break the correct operation of the network. As a result, data-driven network models should be typically trained with data from controlled network testbeds. However, network testbeds are usually much smaller than real networks. In this context, it is essential for our model to effectively scale to larger networks than those seen during the training phase.\\n\\n  \\n\\nIt is well-known that GNN models have an unprecedented capability to generalize over graph-structured. In the context of scaling to larger graphs, it is also known that GNNs keep good generalisation capabilities as long as the spectral properties of graphs are similar to those seen during training. In our particular case, the internal message passing architecture of RouteNet-F generalizes accurately to graphs with similar structures (e.g., a similar number of queues at output ports, or a similar number of flows aggregated in queues). In practice, this means that RouteNet-F should be able to generalize to larger topologies when trained with smaller ones, as long as the networks used for training had the same spectral properties as the larger ones. More details about this can be found in **Section V**, which presents an empirical evaluation of RouteNet-F\'s generalisation capabilities).\\n\\nHowever, scaling to larger networks often entails other aspects beyond the topology size. Two key elements require special attention: link capacities and the range of output variables. First, larger networks naturally have larger link capacities. This in turn results in larger traffic aggregates on core links of the network. Such traffic intensities may fall in ranges not seen in smaller topologies.\\n\\nSecond, as the network scales, it inherently has longer end-to-end paths, which result in increased end-to-end path delays. Again, some of these delays may fall outside the range of delays used when training with smaller topologies. These out-of-range parameters require devising mechanisms to effectively scale on them.\\n\\n**Scaling to larger link capacities**: In RouteNet-F (**Algorithm 1**), the most straightforward way to represent the link capacity $x_{l_c}$ would be as an initial feature of the links\' hidden states $x_l$. However, the fact that $x_{l_c}$ would be encoded as a numerical input value would then introduce inherent limitations to scale to larger capacity values. Indeed, scaling to out-of-distribution numerical values is recognized as a generalized limiting factor among all neural network models.\\n\\nOur approach is to exploit particularities from the network domain to find scale-independent representations for link capacities. These representations define link capacities and how they relate to other link-level features that impact performance (e.g., the aggregated traffic in the link), so they can be used to accurately estimate performance metrics (e.g., delay, jitter, packet loss). Inspired by traditional QT methods, we aim to encode in RouteNet-F the relative ratio between the arrival rates on links (based on the traffic aggregated in the link) and the service times (based on the link capacity). This enables the possibility to infer the output performance metrics of our model from scale-independent values. As a result, instead of directly using the numerical link capacity values, we introduce the link load $x_{l_{load}}$ in the initial feature vector of links $x_{l}$. Particularly, we compute the link load as follows:\\n\\n$$x_{l_{load}} = \\\\frac{1}{x_{l_c}} \\\\sum_{f \\\\in L_f(l_j)} \\\\lambda_f\\\\quad(7)$$\\n\\nWhere $\\\\lambda_f$ is the average traffic volume of the flows that traverse the link $l_j$, and $x_{l_c}$ is the link capacity. In other words, we compute the link load as the summation of all the traffic that would traverse the link without considering possible losses and divide it by the link capacity. Then, through the iterative message-passing process, the GNN model updates the load values after estimating the packet loss.\\n\\n**Different output ranges**: The previous mechanism enables us to keep scale-independent features along with the message-passing phase of our model (loop lines 4-19 in **Algorithm 1**), while it is still needed to extend the scale independence to the output layer of the model. Note that in larger networks, delay values can vary with respect to those seen during the training in smaller networks. This is because flows can go through links with higher capacities, or because flows can potentially traverse longer paths. This again poses the challenge of generalising to ranges of delays not seen in the training phase. Equivalently, this also applies to the prediction of flow jitter and packet loss.\\n\\nTo overcome this potential limitation, RouteNet-F infers delays indirectly from the mean queue occupancy on forwarding devices. Based on traditional QT models, RouteNet-F infers the flow delay as a linear combination of the estimated queuing delays (line 24) and the transmission delays after crossing a link (line 25). Note that a potential advantage with respect to traditional QT models is that the queue occupancy estimates produced by RouteNet-F can be more accurate, especially for autocorrelated and heavy-tail traffic models.\\n\\nWe call the values produced by the $R_{f_d}$ function the effective queue occupancy, which is defined as the mean queue occupancy experienced by a given flow $f_i$ as it passes through a specific forwarding device. More precisely, this value is the average number of bits that have to be served on a specific output port before the packets of flow $f_i$ are transmitted. As an example, let us consider the case of packets from a flow with low priority, which are mapped to low-priority queues. If forwarding devices implement a multi-queue Strict Priority scheduling policy, the effective queue occupancy seen by those low-priority packets should include all the bits to be served in the queues with higher priority.\\n\\nThe prediction of this effective queue occupancy - instead of directly predicting delays - helps overcome the practical limitation of producing out-of-range delay values with the readout function $R_{f_d}$. In this case, the values produced by $R_{f_d}$ are bounded between 0 and the maximum buffer size at the output ports of forwarding devices. Note that the buffer size is a device-specific feature that is independent of the network size.\\n\\nLastly, RouteNet-F produces flow-level delay predictions $\\\\hat{y_{f_d}}$ by combining the estimated queuing and transmission delays. The queuing delay $\\\\hat{d_q}$ is indirectly estimated by using the effective queue occupancies (in bits) on queues for a particular flow. Particularly, queue occupancy values are estimated by the readout function $R_{f_d}(h^T_{f,l})$. Then, they are divided by the capacity of the link connected to the output port $x_{l_c}$ to eventually produce a queuing delay estimate $\\\\hat{d_q}$. Likewise, the transmission delay $\\\\hat{d_t}$ is computed by dividing the mean flow packet size $x_{f_{ps}}$ by the link capacity $x_{l_c}$. With this, RouteNet-F estimates the delay of a flow after passing through a specific forwarding device and a link ($\\\\hat{d_{link}}$):\\n\\n$$\\\\hat{d_q} = \\\\frac{R_{f_d}(h^T_{f,l})}{x_{l_c}}\\\\quad(8)$$\\n$$\\\\hat{d_t} = \\\\frac{x_{f_{ps}}}{x_{l_c}}\\\\quad(9)$$\\n$$\\\\hat{d_{link}} = \\\\hat{d_q} + \\\\hat{d_t}\\\\quad(10)$$\\n\\nHence, we can compute end-to-end flow delays as the sum of all the link delays $\\\\hat{d}_{link}$ along the flow (loop lines 20-27 in **Algorithm 1**). Note that the function responsible for computing the effective queue occupancy, $R_{f_d}(h^T_{f,l}$) takes as input the hidden state of the flow at a specific link $h^T_{f,l}$, instead of directly considering queue states $h^T_{q}$. This is because each flow can experience a different queuing behavior depending on its properties (e.g., traffic burstiness).\\n\\nLikewise, jitter estimates $\\\\hat{y}_{f_j}$ are produced by combining the jitter predictions of all links along the flow. These predictions are made by the $R_{f_j}$ function, which takes as input the hidden state of the flow at a specific link $h^T_{f,l}$. Note that we define the jitter as the relative fluctuation with respect to the mean delay, that is, the ratio between the delay variance divided by the flow mean delay.\\n\\nIn the case of packet loss, RouteNet-F makes predictions directly on flows\' hidden states $h_f^T$. We define the packet loss as the relative ratio of packets dropped with respect to the packets transmitted by the source; hence it is a bounded value $\\\\hat{y_{f_l}}\\\\in[0,1]$. We estimate it with the $R_{f_l}$ function (line 29 in **Algorithm 1**).\\n\\n### Training and Implementation\\n\\nWe implement RouteNet-Fermi in TensorFlow and it is publicly available at [Github](https://github.com/BNN-UPC/Papers/wiki/RouteNet_Fermi).\\n\\nAs in any other ML model, fine-tuning the hyper parameters of RouteNet-F is crucial for achieving optimal performance and accuracy. Two key parameters to consider are the hidden state vectors ($h_f$, $h_q$, $h_l$) and the number of message passing iterations ($T$). The size of the hidden state vectors determines how much information the model can encode, with larger sizes allowing for more information but also harming performance. Similarly, a larger number of message-passing iterations can help the model reach a higher level of convergence, but at the cost of increased computation. Through grid search experiments, we selected a set of hyper parameters that provide good accuracy while also being efficient. Specifically, we set the size of all the hidden state vectors to 32 elements and $T$ to 8 message-passing iterations.\\n\\nWe implement the functions $FRNN$ (Flow-Level RNN), $LRNN$ (Link-Level RNN), and $U_q$ (Queue Update Function) as Gated Recurrent Units (GRU). Functions $HS_f$, $HS_q$, and $HS_l$ are implemented as 2-layer fully-connected neural networks with ReLU activation functions with 32 units each. Similarly, functions $R_{f_d}$, $R_{f_j}$, and $R_{f_l}$ are implemented as a 3-layer fully-connected neural networks with ReLU activation function for the hidden layers, and a linear one for the output layer (except for $R_{f_l}$ that uses a Sigmoid activation function). Note that, the whole architecture of RouteNet-F (**Algorithm 1**) constitutes a fully-differentiable function. This means that it can be trained end-to-end using as input the network samples and as output the different flow-level performance metrics (e.g., delay, jitter, packet loss) as illustrated in the black box diagram of **Figure 1**.\\n\\nFor each set of experiments, we train RouteNet-F during 150 epochs of 2,000 samples each. We set as loss function the Mean Absolute Percentage Error for the delay experiments, the Mean Squared Error for jitter, and the Mean Absolute Error for the packet loss. In all the cases, we use an Adam optimiser with an initial learning rate of 0.001.\\n\\n## Evaluation\\n\\nIn this section, we evaluate the performance of RouteNet-F in a wide range of relevant scenarios. First, we evaluate RouteNet-F in a variety of scenarios with complex traffic models and scheduling policies and compare it to a state-of-the-art Queuing Theory (QT) benchmark from RouteNet-Erlang. In it, the network is modeled as a $M/M/1/b$ system where each queue along a path is treated independently. Note that the baseline, like the majority of QT models, assumes that arrivals to each queue are approximated by a Poisson process and that service times are exponentially distributed. Under these assumptions, the model derives analytical results for queue throughput, delay distributions, and blocking probabilities. A public implementation of the QT model can be found at [Queunix](https://github.com/krzysztofrusek/queuinx).\\n\\nThen, we evaluate RouteNet-F\'s generalisation capabilities when evaluated in topologies x30 times larger than the ones seen during training and compare its inference times. Finally, we benchmark RouteNet-F in real-world scenarios with data from a real testbed, with traffic coming from real-world networks, and compare its performance with MimicNet a state-of-the-art DL-based model.\\n\\n### A. Performance Analysis\\n\\n#### Methodology\\n\\nIn all the experiments (except for **subsection V-C1** that comes from a real testbed), the ground truth is obtained using a packet-level simulator (**Section V-A2**). Unless specified, in each evaluation we perform 50k experiments with a random configuration (src-dst routing, traffic intensity, per-interface scheduling policy, queue size, and traffic model), and compute the mean average delay, jitter, and packet loss. Then we compute the error of RouteNet-F\'s and QT\'s estimates with respect to the results of the packet simulator. For a fair comparison, the evaluation samples have not been used in the training phase of RouteNet-F.\\n\\n#### Dataset\\n\\nWe generate our dataset by simulating a wide range of network scenarios with the OMNet++ network simulator, v5.5.1. An image of the simulator is publicly available and can be found at [BNNetSimulator](https://github.com/BNN-UPC/BNNetSimulator). Each dataset sample corresponds to a single simulation, and we record the mean delay, jitter, and packet loss for all the flows in the network, as well as queue-level statistics(e.g., mean occupancy, average packet loss, or average packet size). We select the different scenarios to simulate by randomly sampling from the possible values of all the input variables (**Table IV**). The traffic models autocorrelated exponentials and modulated exponentials reproduce realistic Internet traffic. We define the traffic intensity ($TI$) as a tunable parameter that defines the overall traffic load in the network scenario. $TI$ represents how congested is the network. In our dataset, it ranges from 400 to 2000 bits per time unit, with 400 being the lowest congested network (0% avg. packet loss) and 2000 a highly congested network ($\\\\approx$ 3% avg. packet loss).\\n\\n| Simulation Variables  |                                                                                                                                                              |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| Topology              | NSFNET, GEANT, GBN, and scale-free synthetic topologies following the Power-Law Out-Degree algorithm.                                                        |\\n| Traffic Model         | 6 options: Poisson, On-Off, Constant Bitrate, Autocorrelated Exponentials, Modulated Exponentials (according to RouteNet-Erlang), and all models mixed.      |\\n| Traffic Intensity     | Random traffic intensities to generate packet loss between 0% and 3%.                                                                                        |\\n| Queuing Configuration | 1, 2, or 3 queues per port. Queue size: 8, 16, 32, or 64 kbits. Policy: First In First Out, Strict Priority, Weighted Fair Queuing, and Deficit Round Robin. |\\n\\n*Table. IV Simulation Variables*\\n\\n#### Traffic Models\\n\\nThis section analyses the accuracy of RouteNet-F in a wide range of traffic models. The experiment is organised such that, for each traffic model, we add a degree of complexity by changing its first and second-order statistics (i.e., variance and autocorrelation). We start with simple traffic models such as Poisson or Constant Bitrate and end by testing more complex models that are better approximations of traffic seen in Internet links.\\n\\n**Tables V** and **VI** show the errors of the delay and jitter for both, RouteNet-F and QT, with respect to the values obtained using the simulator. We can see that RouteNet-F achieves excellent accuracy results, producing very accurate estimates of delay and jitter in all traffic models: the worst cases are 5.21% and 10.40% for delay and jitter, respectively.\\n\\n| QT              |       |       |       |       |\\n| --------------- | ----- | ----- | ----- | ----- |\\n|                 | MAPE  | MSE   | MAE   | $R^2$ |\\n| Poisson         | 12.6% | 0.001 | 0.017 | 0.998 |\\n| Deterministic   | 22.4% | 0.715 | 0.321 | 0.611 |\\n| On-Off          | 23.1% | 0.784 | 0.363 | 0.613 |\\n| A. Exponentials | 21.1% | 0.686 | 0.361 | 0.618 |\\n| M. Exponentials | 68.1% | 1.10  | 0.798 | 0.145 |\\n| Mixed           | 35.1% | 0.721 | 0.430 | 0.560 |\\n\\n*Table. V (QT) Delay prediction using the QT baseline and RouteNet-F for different traffic models. The error is computed w.r.t. simulation results.*\\n\\n| RouteNet-F      |       |       |       |       |\\n| --------------- | ----- | ----- | ----- | ----- |\\n|                 | MAPE  | MSE   | MAE   | $R^2$ |\\n| Poisson         | 2.1%  | 0.001 | 0.017 | 0.999 |\\n| Deterministic   | 4.43% | 0.029 | 0.048 | 0.984 |\\n| On-Off          | 2.90% | 0.009 | 0.035 | 0.995 |\\n| A. Exponentials | 2.62% | 0.010 | 0.030 | 0.994 |\\n| M. Exponentials | 5.21% | 0.013 | 0.061 | 0.989 |\\n| Mixed           | 4.71% | 0.018 | 0.054 | 0.988 |\\n\\n*Table. V (RouteNet-F) Delay prediction using the QT baseline and RouteNet-F for different traffic models. The error is computed w.r.t. simulation results.*\\n\\n| QT              |       |       |       |        |\\n| --------------- | ----- | ----- | ----- | ------ |\\n|                 | MAPE  | MSE   | MAE   | $R^2$  |\\n| Poisson         | 71.9% | 0.013 | 0.072 | 0.849  |\\n| Deterministic   | 99.0% | 0.057 | 0.067 | -1.86  |\\n| On-Off          | 69.4% | 0.057 | 0.098 | 0.425  |\\n| A. Exponentials | 74.3% | 0.025 | 0.067 | 0.246  |\\n| M. Exponentials | 91.4% | 1.34  | 0.834 | -0.622 |\\n| Mixed           | 69.1% | 0.299 | 0.296 | 0.025  |\\n\\n*Table. VI (QT) Jitter prediction using the QT baseline and RouteNet-F for different traffic models. The error is computed w.r.t. simulation results.*\\n\\n| RouteNet-F      |       |       |       |       |\\n| --------------- | ----- | ----- | ----- | ----- |\\n|                 | MAPE  | MSE   | MAE   | $R^2$ |\\n| Poisson         | 6.26% | 0.001 | 0.013 | 0.980 |\\n| Deterministic   | 7.17% | 0.001 | 0.008 | 0.924 |\\n| On-Off          | 8.50% | 0.004 | 0.018 | 0.959 |\\n| A. Exponentials | 6.29% | 0.001 | 0.008 | 0.973 |\\n| M. Exponentials | 10.3% | 0.036 | 0.091 | 0.956 |\\n| Mixed           | 9.82% | 0.007 | 0.034 | 0.974 |\\n\\n*Table. VI (RouteNet-F) Jitter prediction using the QT baseline and RouteNet-F for different traffic models. The error is computed w.r.t. simulation results.*\\n\\nAs expected, the estimates of the QT model are unacceptable in continuous-state traffic models, e.g. almost 70\\\\% for modulated exponentials. On the other hand, it achieves moderate accuracy for discrete-state models (Poisson, Deterministic, and On-Off). It is also noticeable how the QT model shows poor accuracy across all the jitter estimations. The main reason for this is that QT assumes independence between queues in the network. Hence, the estimator used to compute the jitter is the sum of the individual delay variance of queues along the flow\'s paths.\\n\\nFor non-Markovian traffic models (e.g., On-Off), RouteNet-F produces accurate estimates, as well as for more challenging models that implement strong autocorrelation (Autocorrelated Exponentials) and heavy-tail distributions (Modulated Exponentials). These models are important since they approximate real traffic generated by TCP, similar to that found at Internet links. Also, notice that this traffic model could be made even more difficult for QT by increasing both the variance and the autocorrelation factor.\\n\\nWe add a final scenario (Mixed) where each src-dst pair generates traffic by randomly selecting one of the five available traffic models, and using random parameters for these models. In other words, we multiplex all traffic models in a single network topology. As the table shows, RouteNet-F shows good accuracy not only when modeling individual traffic models, but also when they are mixed across links in the network. It is worth noting that although RouteNet-F shows excellent accuracy for arbitrary parameterisations of these 6 traffic models, it cannot generalize to new traffic models not introduced during the training phase.\\n\\nFinally, **Table VII** shows the different metrics for the packet loss ratio. Since there are some paths where the packet loss ratio is zero, we provide the Mean Absolute Error and the Coefficient of Determination ($R^2$). The packet loss ratio is measured as the percentage of packets dropped w.r.t. packets sent, that is why the MAE is expressed in % units. In this particular case, it is noticeable how the QT baseline works well in the scenario with the Poisson traffic model. However, the accuracy decreases remarkably in more complex scenarios. On the other hand, RouteNet-F obtains a high accuracy with a worst-case MAE of 1.2% and $R^2\\\\geq 0.98$.\\n\\n|                 |       | QT   | RouteNet-F |\\n| --------------- | ----- | ---- | ---------- |\\n| Poisson         | MAE   | 1.0% | 0.3%       |\\n|                 | $R^2$ | 0.97 | 0.99       |\\n| Const. Bitrate  | MAE   | 11%  | 1.0%       |\\n|                 | $R^2$ | 0.64 | 0.99       |\\n| On-Off          | MAE   | 9.5% | 1.0%       |\\n|                 | $R^2$ | 0.63 | 0.99       |\\n| A. Exponentials | MAE   | 10%  | 1.2%       |\\n|                 | $R^2$ | 0.65 | 0.99       |\\n| M. Exponentials | MAE   | 9.5% | 1.1%       |\\n|                 | $R^2$ | 0.08 | 0.98       |\\n| Multiplexed     | MAE   | 4.6% | 0.50%      |\\n|                 | $R^2$ | 0.50 | 0.99       |\\n\\n*Table. VII Packet Loss evaluation - Mean Absolute Error and Coefficient of Determination ($R^2$) of QT and RouteNet-F for the different traffic models.*\\n\\n#### Scheduling\\n\\nThis section aims to validate if RouteNet-F is capable of modeling the behavior of queues in the presence of several scheduling policies. For this purpose, we train the model using samples with mixed queue scheduling policies across nodes in the GEANT and NSFNET topologies. Then, we evaluate the model on samples of the GBN topology (unseen during training). In this experiment, each router port implements three different queues with a randomly selected scheduling policy for the queues: $(i)$ First In, First Out (FIFO), $(ii)$ Weighted Fair Queueing (WFQ), $(iii)$ Deficit Round Robin (DRR), and $(iv)$ Strict Priority (SP). For WFQ and DRR, the set of weights is also randomly selected. Furthermore, each flow has been assigned a Quality-of-Service class that maps it to a specific queue depending on the flow priority. To provide a fair benchmark with QT, in this experiment we use only Poisson traffic.\\n\\n**Table VIII** shows the Mean Average Percentage Error (MAPE) of the delay and jitter for three different traffic intensities: from low-loaded to highly-congested scenarios. According to [this paper](https://ieeexplore.ieee.org/abstract/document/944338) the average packet loss on the Internet is around 2%-3%. Based on this, in the highly-congested scenarios, the mean packet loss rate is around 3% which we believe represents a wide range of realistic network scenarios. We can see that RouteNet-F outperforms the QT benchmark in both metrics (delay and jitter), obtaining a MAPE of 3.57% for delay and 8.17% for jitter after averaging the results over the three traffic intensities.\\n\\n|            | Delay |        |       | Jitter |        |       |\\n| ---------- | ----- | ------ | ----- | ------ | ------ | ----- |\\n|            | Low   | Medium | High  | Low    | Medium | High  |\\n| QT         | 13.0% | 17.3%  | 25.1% | 49.0%  | 53.2%  | 59.6% |\\n| RouteNet-F | 0.80% | 2.60%  | 7.31% | 3.95%  | 5.77%  | 14.8% |\\n\\n*Table. VIII Delay and jitter evaluation - Mean Absolute Percentage Error of QT and RouteNet-F in the presence of Scheduling Policies for low, medium, and high traffic intensity.*\\n\\nSimilarly, **Table IX** presents the results for packet loss. Again, RouteNet-F outperforms the QT benchmark showing an MAE of 0.7% and an average $R^2$ close to 0.99.\\n\\n|            | Low   |       | Medium |       | High  |       |\\n| ---------- | ----- | ----- | ------ | ----- | ----- | ----- |\\n|            | MAE   | $R^2$ | MAE    | $R^2$ | MAE   | $R^2$ |\\n| QT         | 4.55% | -0.05 | 9.22%  | 0.00  | 10.6% | 0.29  |\\n| RouteNet-F | 0.2%  | 0.97  | 0.10%  | 0.99  | 0.40% | 0.99  |\\n\\n*Table. IX Packet loss evaluation - Mean Absolute Error and Coefficient of Determination ($R^2$) of QT and RouteNet-F in the presence of Scheduling Policies for low, medium, and high traffic intensity.*\\n\\n### B. Generalization to larger networks"},{"id":"welcome","metadata":{"permalink":"/blog/welcome","source":"@site/blog/2021-08-26-welcome/index.md","title":"Awesome things to do","description":"Computer science is such a powerful tool, which allows developers do incredible things.","date":"2021-08-26T00:00:00.000Z","tags":[{"label":"Plans","permalink":"/blog/tags/plans"}],"readingTime":0.065,"hasTruncateMarker":false,"authors":[{"name":"Milo Song","title":"Independent Developer","url":"https://mi-x-lab.github.io","imageURL":"https://github.com/mi-x-lab.png","key":"myself"}],"frontMatter":{"slug":"welcome","title":"Awesome things to do","authors":"myself","tags":["Plans"]},"unlisted":false,"prevItem":{"title":"RouteNet-Fermi, Network Modeling with Graph Neural Networks","permalink":"/blog/RouteNet-Fermi/RouteNet-Fermi"}},"content":"Computer science is such a powerful tool, which allows developers do incredible things."}]}')}}]);